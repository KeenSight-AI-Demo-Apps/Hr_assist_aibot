# ğŸ§  HR Assist AI - Chatbot for HR Benefits FAQs  
This project is a local-first, privacy-friendly HR chatbot designed to handle employee questions about health benefits and HR policies using company-specific documents. It uses Retrieval-Augmented Generation (RAG) powered by LlamaIndex, Ollama, and OpenWebUI, all running inside Docker.

---

## ğŸ“ Project Structure  

```
hr_assist_ai/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ plugins/
â”‚   â”‚   â””â”€â”€ hr_plugin.py           # Main plugin logic (LlamaIndex RAG)
â”‚   â””â”€â”€ custom_apps/
â”‚       â””â”€â”€ data_loader.py         # Used for indexing CSVs or other docs
â”œâ”€â”€ data/
â”‚   â””â”€â”€ hr_benefits.csv            # Your HR FAQ file (source for chatbot)
â”œâ”€â”€ storage/                       # Vector store & index persistence
â”œâ”€â”€ open-webui/                    # Cloned UI interface (via `git clone`)
â”œâ”€â”€ docker-compose.yml             # Runs OpenWebUI + Ollama containers
â””â”€â”€ README.md                      # This file
```
---

## ğŸ› ï¸ Technologies Used  
| **Purpose**                | **Tool**                | **Description**                                    |
|---------------------------|-------------------------|----------------------------------------------------|
| LLM Backend               | Ollama + llama3         | Local large language model                         |
| Embeddings                | mxbai-embed-large       | Local embedding model                              |
| RAG & Indexing            | LlamaIndex (0.12.47)    | Handles document parsing and vector store          |
| Chat Interface            | OpenWebUI               | User-facing intranet chatbot                       |
| Containerization          | Docker + docker-compose | To run everything locally                          |
| Programming Language      | Python 3.11             | Core application logic                             |

---

## ğŸ” Setup & Installation  

**1. Clone the Repo**  
```bash
git clone https://github.com/open-webui/open-webui.git
cd hr_assist_ai
```
Make sure open-webui/ folder exists inside hr_assist_ai/.

**2. ğŸ Install Python Dependencies (in .venv)**  
```bash
python -m venv .venv
.venv\Scripts\activate    # Use `source .venv/bin/activate` on Linux/macOS
pip install llama-index langchain sentence-transformers pandas
pip install llama-index-llms-ollama llama-index-embeddings-ollama
```

**3. Prepare Documents**  
Put your HR documents (e.g., FAQs, benefit plans) inside the `data/` folder. Start with `hr_benefits.csv`.

**3. Install & Pull Ollama Models**  
Install Ollama and pull required models:  
```bash
ollama pull llama3  
ollama pull mxbai-embed-large  
```

**4. Start Docker Containers** 
ğŸ“¦ Docker Setup
Make sure Docker is installed and running. 
```bash

docker-compose up -d --build 
```  
This launches:  
- Ollama at `http://localhost:11434`  
- OpenWebUI at `http://localhost:3000`

**5. (Optional) Rebuild Containers**  
If you change anything in `docker-compose.yml` or `Dockerfile`:  
```bash
docker-compose down -v  
docker-compose up -d --build  
```

---

## ğŸ§  How It Works  

- **Data Ingestion**: `data_loader.py` loads CSVs and persists them to vector index using `mxbai-embed-large` embeddings.  
- **Query Processing**: `hr_plugin.py` loads the index and runs a retrieval + LLM-based response pipeline.  
- **User Interface**: OpenWebUI provides a chat interface. You connect it with `hr_plugin.chat(query)` so user questions are answered from your HR data.

---

## ğŸ§ª Running in OpenWebUI (In-Browser)  

1. Open `http://localhost:3000`  
2. Start a new chat  
3. Ask questions like:  
   > What health benefits do employees receive?

âš ï¸ If RAG is not working in the browser yet, test it using this (inside the container):  
```bash
docker exec -it hr_assist_ai-open-webui sh  
export PYTHONPATH=/app  
python3  
>>> from backend.plugins.hr_plugin import main 
>>> main("What are health benefits?")  
```

---

## ğŸ” Known Issues & Fixes  

âœ… **Fixed**  
- `llama_index` module errors by using correct version (0.12.47)  
- Ensured Ollama and OpenWebUI both access models locally  
- Integrated `mxbai-embed-large` for embeddings  
- Container networking issues fixed by aligning Docker and Ollama ports  

â—**Pending**  
- Ensuring OpenWebUI can call `hr_plugin.py` backend directly  
- Confirming RAG answers display in UI not just via CLI  

---

## ğŸš€ What's Next  

- Integrate file upload into OpenWebUI chat  
- Allow multiple document types (PDF, DOCX)  
- Add chat history per employee/user  
- Add HR policy documents, SPDs, and medical plan PDFs  

---

## ğŸ¤ Contribution  
Pull requests welcome! This is built for local HR teams wanting private, document-grounded AI without cloud APIs.